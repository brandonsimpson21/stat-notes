{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joint Distributions\n",
    "\n",
    "$$p(A,B)=p(A|B)p(B)$$\n",
    "\n",
    "### Marginal Distrubtion\n",
    "\n",
    "given a joint distribution we define the marginal distribution as\n",
    "\n",
    "$$p(A)=\\sum_bp(a,b)=\\sum_bp(A|B=b)\\;p(B=b)$$\n",
    "\n",
    "### Chain Rule of Probability\n",
    "\n",
    "$$p(x_{1:D})=p(X_1)p(x_2|X_1)p(X_3|X_2,X_1)\\cdots p(X_D|X_{1:d-1})$$\n",
    "\n",
    "### Conditional Probability\n",
    "\n",
    "$$p(A|B)=\\frac{p(A,B)}{p(B)}$$ if $B>0$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Bayes-Rule)=\n",
    "## Bayes Rule\n",
    "\n",
    "$$p(X=x|Y=y)=\\frac{p(X=x)p(Y=y)}{p(Y=y)})=\\frac{p(X=x)pY=y|X=x)}{\\sum_{x^\\prime{}}p(X=x^\\prime)p(Y=y|X=x^\\prime)} $$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Independence & Conditional Independence\n",
    "\n",
    "$X$ and $Y$ are unconditionall independent or marginally independent denoted $X\\perp Y$ if ew can represent the joint as the product of the two marginals\n",
    "\n",
    "$$X\\perp Y \\iff p(x,y)=p(X)p(Y)$$\n",
    "\n",
    "we say $X$ and $Y$ are conditionall independent given $Z$ iff the conditional joint can be written as the product of conditional marginals\n",
    "\n",
    "$$X \\perp Y |Z \\iff p(X,Y|Z)=p(X|Z)p(Y|Z)$$\n",
    "\n",
    "it is also true that $X\\perp Y |Z$ iff there exist functions $g$ and $h$ such that\n",
    "\n",
    "$$p(x,y|z)=g(x,z)h(y,z)\\; \\forall \\; x,y,z \\;st\\;p(z)>0$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Covariance And Correlation\n",
    "\n",
    "the covariance between two rv's $X$ and $Y$ measures the degree to which $X$ and $Y$ are linearly related\n",
    "\n",
    "$$cov[X,Y]\\triangleq \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}Y)]=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]$$\n",
    "\n",
    "if $x$ is a d-dimensional random vector its covariance matriz is defined by the following symmetric positive definite matrix\n",
    "\n",
    "\\begin{align}\n",
    "    cov[x] &\\triangleq \\mathbb{E}\\left[(x-\\mathbb{E}[x])(x-\\mathbb{E}[x])^T\\right]\\\\\n",
    "    &=\n",
    "\\end{align}\n",
    "\n",
    "$0\\leq cov\\leq \\inf$\n",
    "\n",
    "sometimes its easier to work with a normalized measure with a finite upper bound the pearson correlation coefficient between $X$ and $Y$ is\n",
    "\n",
    "$$corr[X,Y] \\triangleq \\frac{cov[X,Y]}{\\sqrt{var[X]var[Y]}}$$ where $-1\\leq corr[X,Y] \\leq 1 $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformations of Random Variables\n",
    "\n",
    "if $x \\sim p()$ is some rv and $y=f(x)$ what is the distribution of $y$? we can use the change of variable formula\n",
    "\n",
    "$$p_y(y)=p_x(x)\\left|\\frac{dx}{dy}\\right|$$\n",
    "\n",
    "for multivariate change of variables\n",
    "\n",
    "let $f$ be a function that maps $\\mathbb{R}^n$ to $\\mathbb{R}^n$ and let $y=f(x)$ than its jabobian matrix $J$ is given by\n",
    "\n",
    "$$J_{x\\to y} \\triangleq \\frac{\\partial(y_y, \\cdots ,y_n)}{\\partial{(x_1, \\cdots , x_n)}} \\triangleq \\begin{bmatrix}\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_n}{\\partial x_1} & \\cdots & \\frac{\\partial y_n}{\\partial x_n}  \\end{bmatrix}$$\n",
    "\n",
    "if $f$ is an invertible mapping\n",
    "\n",
    "$$p_y(y)=p_x(x)|detJ_{y \\to c}|$$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entropy\n",
    "\n",
    "the entropy of a rv $X$ with distribution $p$ denoted $\\mathbb{H}(X)$ for a discrete distribution with $K$ states\n",
    "\n",
    "$$\\mathbb{H}(X) \\triangleq -\\sum_{k=1}^K p(X=k)log_2p(X=k)$$\n",
    "\n",
    "the discrete distribution with maximum entropy is the uniform distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(KL-Divergence)=\n",
    "## Kullback-Leibler Divergence\n",
    "\n",
    "aka relative entropy\n",
    "\n",
    "$$\\mathbb{KL}(p||q) \\triangleq \\sum_{k=1}^Kp_klog\\frac{p_k}{q_k} = -\\mathbb{H}(p) + \\mathbb{H}(p,q)$$\n",
    "\n",
    "where $\\mathbb{H}(p,q) \\triangleq -\\sum_{k}p_klog(q_k)$ is the cross entropy\n",
    "\n",
    "theorem: $\\mathbb{KL}(p||q)  \\geq 0$ with equality iff $p=q$ proof involves jensens inequality\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mutual Information\n",
    "\n",
    "let $x,y$ be random variables we want to determine how similar $p(y,x$ is to the factored distribution $p(x)p(y)$\n",
    "\n",
    "$$\\mathbb{I}(x;y) \\triangleq \\mathbb{KL}(p(x,y)||p(x)p(y)) =\\mathbb{H}(y)-\\mathbb{H}(y|x)= \\sum_x\\sum_yp(x,y)log\\frac{p(x,y)}{p(x)p(y)}$$\n",
    "\n",
    "\n",
    "MI = 0 if variables are independent\n",
    "\n",
    "### pointwise mutual information\n",
    "\n",
    "$$PMI(x,y) \\triangle log\\frac{p(x,y)}{p(x)p(y)} = log \\frac{p(y|x)}{p(y)}$$\n",
    "\n",
    "we can rewrite this $PMI(x,y) = log \\frac{p(y|x)}{p(y)}$ this is the amount we learn from updating the prior $p(x)$ to the posterior $p(x|y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Markov Inequality\n",
    "\n",
    "let $x$ be a random variable and $u(x)$ be a non negative function than for any $c>0$\n",
    "\n",
    "$$p(u(x) \\geq c) \\leq \\frac{\\mathbb{E}[u(x)]}{c}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chebyshevs Inequality\n",
    "\n",
    "let $x$ be a random variable with mean $\\mu$ and variance $\\sigma^2<\\infty$ then\n",
    "\n",
    "$$p(|x-\\mu|) \\geq k\\sigma) \\leq \\frac{1}{k^2}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Jensens Inequality\n",
    "\n",
    "if $\\phi(x)$ is convex on an interval $I$ and the support of $x$  is in $I$\n",
    "\n",
    "$$\\phi(\\mathbb{E}[x]) \\leq \\mathbb{E}(\\phi(x)]$$ provided the expecations exist"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}