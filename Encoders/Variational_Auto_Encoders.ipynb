{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Let $x$ be the data we want to model and $z$ be a latent variable\n",
    "\n",
    "$$p(x)=\\int p(x|z)p(z)dz$$\n",
    "\n",
    "A VAE tries to infer $p(z)$ using $p(z|x)$ which we have to infer, say we want to infer $p(z|x)$ using $Q(z|x)$ the {ref}'KL-Divergence' becomes\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{KL}[Q(z|x)||p(z|x)] &= \\sum_zq(z|x)ln\\left(\\frac{q(z|x)}{p(z|x)}\\right) \\tag{1}\\\\\n",
    "&=E\\left[ln\\left(\\frac{q(z|x)}{p(z|x)}\\right)\\right] \\tag{2}\\\\\n",
    "&=E\\left[ln(q(z|x))-ln(p(z|x))\\right]\\tag{3}\n",
    "\\end{align}\n",
    "\n",
    "using {ref}'Bayes-Rule' at step 2 above you get\n",
    "\n",
    "\\begin{align}\n",
    "E\\left[ln\\left(\\frac{q(z|x)}{p(z|x)}\\right)\\right]&=E\\left[ln(q(z|x))-ln(p(x|z))+ln(p(z))-ln(p(x))\\right]\\\\\n",
    "&=E\\left[ln(q(z|x))-ln(p(x|z))-ln(p(z))+ln(p(x))\\right]\n",
    "\\end{align}\n",
    "\n",
    "since the expectation is over $z$ and $p(x)$ doesn't depend on z we can move it out of the expectation and to the other side of the equality\n",
    "\n",
    "$$\\mathbb{KL}[Q(z|x)||p(z|x)]-ln(p(x))=E\\left[ln(q(z|x))-ln(p(x|z))-ln(p(z))\\right]$$\n",
    "\n",
    "using linearity of expection and 3 above we can rewrite the right hand side as another $\\mathbb{KL}$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{KL}[Q(z|x)||p(z|x)]-ln(p(x))&=E\\left[ln(q(z|x))-ln(p(x|z))-ln(p(z))\\right]\\\\\n",
    "ln(p(x))-\\mathbb{KL}[Q(z|x)||p(z|x)] &=E\\left[ln(p(x|z)-(ln(q(z|x)-ln(p(z))\\right]\\\\\n",
    "&= E[ln(p(x|Z))-\\mathbb{KL}(q(z|x)||p(z))\n",
    "\\end{align}\n",
    "\n",
    "this is the VAE objective function where $q(z|x)$ projects $x$ into latent variable space\n",
    "1. $q(z|x)$ is the encoder net\n",
    "2. $z$ is the encoded representation\n",
    "3. $p(x|z)$  is the decoder net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we want to model $p(x)$ under some error $\\mathbb{KL}(q(z|x)||p(z|X))$ we try to find the lower bound of $ln(p(x))$\n",
    "the model is found over some mapping from latent variable to  data $ln(p(x|Z))$ and minimizing difference between $q(z|x)$ and $p(z)$\n",
    "\n",
    "maximizing $E[ln(p(x|Z))]$ is an MLE estimate\n",
    "\n",
    "we try to make $q(z|x)\\approx \\mathbb{N}(0,1)$ since that models $p(z)$ if we are standard normal $\\mu(x),\\Sigma(x)$ $\\mathbb{KL}$ can be written in closed form\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{KL}(\\mathbb{N}(\\mu(x),\\Sigma(x)||\\mathbb{N}(0,1))&=\\frac{1}{2}(tr(\\Sigma(x))+\\mu(x)^T\\mu(x)-k-log(det(\\Sigma(x)))\\\\\n",
    "&=\\frac{1}{2}\\sum_k(exp(\\Sigma(x))+\\mu^2(x)-1-\\Sigma(x))\n",
    "\\end{align}"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}